{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition:\n",
    "\n",
    "Recently we have been witnesses of the massive hype around cryptocurrencies used as an investment asset rather than a currency as well, considering that from 2013 until now, there has been a bias regarding the trends in their price and real value. For instance, if we compare the price of the most famous cryptocurrency nowadays on the market (and the first one), Bitcoin, in different long-time intervals, an enormous difference is perceived between them. While in May 2013 its price never surpassed the $ 150 (US currency) threshold, in December 2017 its price reached its peak with $ 19.535,70.\n",
    "\n",
    "Such kind of facts, generate a significant misunderstanding about the risks and volatility implied when dealing with these markets, where the less financial educated population are the most vulnerable. \n",
    "\n",
    "Precedents in Latin America countries, including Brazil and Colombia, could be pointed out using examples like when Forex market was widely introduced and began to operate in these countries. Several intermediate fraud schemas rised (including pyramidal schemas) intending to collect population money using speeches based on biased facts such as the hyperinflation of bitcoin price mentioned above. The success of this schemas was caused mainly because of the effectiveness of their speeches empowered by the lack of financial education of the targeted population.\n",
    "\n",
    "Consequently, similar fraud schemas based on cryptocurrencies speculation investment born in this case due either the lack of knowledge in finance and/or cryptocurrencies concept and the gaps existing due the absence of policies and laws to regulate such markets regardless the attempts encouraged by countries like China, Japan, Sweden and others, of framing out an efficient regulatory policy in this matter. \n",
    "\n",
    "Furthermore, it is clear that even for more educated population, is still hard to evaluate and have a broader sight about the real potential risks and gains when investing in these currencies in certain markets such as the Brazilian one. Consequently, this work aims to overcome the problems pointed above by delivering clearer evidences regarding the real behavior of these assets in diversified portfolios using “Efficient frontier Markowitz model”, evolutionary-based algorithms and VaR metric (Value at Risk).\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The history regarding financial markets stands over several failures which led to important advances in the matter for the sake of failure prevention and smarter decisions. In 1987 for instance, soon after the stock market crash, the lack of means and tools for measuring the potential risk of an asset under certain market conditions led to the development of Value at Risk (VaR) and since then has seen widespread use across financial services organizations until it was standardized in 1999 by the Basel agreement (eafit) [spark analytics].\n",
    "\n",
    "\n",
    "``(Still workin progress .. Even in earlier stages, theory regarding )\n",
    "\n",
    "However regardless such advances, erroneous behaviors still persist and it was the case of 2008’s crisis where the excess of confidence across the investors and regulatory intuitions mainly, leveraged by the constant growth and great returns the market offered for decades led to the bigger financial crisis in the modern time[eafit].\n",
    "\n",
    "Along with this happening but independently, advances in technology with the capability of defining the economic scenario in the years to come were being developed. That is the case of Blockchain, the technology behind the most famous cryptocurriencies, where the later ones (Bitcoin specifically) were the first succeed implemented applications of Blockchain in 2008. \n",
    "\n",
    "``\n",
    "\n",
    "It is important to remark this work deals with cryptocurrencies as financial assets only. Technical details regarding how the technology behind these assets works, is out of the scope.\n",
    "\n",
    "After these events, with new players in the economy and considering the non-stable scenario fed by the past crisis, turned the market more volatile and hard to predict, thus surviving such contexts requires more sophisticated tools and strategies to manage investments more “correctly”. \n",
    "\n",
    "From traditional invesemtent models: \n",
    "\n",
    "\n",
    "According to (EAFIT reference), the most known investement startegies are:\n",
    "\n",
    "** The equal wiehgt strategy:** Is one of the most used strategy in portfolio diversification due its simplicity in implementation and understading. It simply weights every stock in a portfolio equally.\n",
    "\n",
    "** Minimum variance strategy:** Based on Markoweitz portfolio optimizatoin model, from the optimizied portfolios set, the portafolio with the minimum value of variance is selected.\n",
    "\n",
    "** Mean variance straegy:** Once the Markowitz efficient frontier is calculated, Ratio of Sharpe will lead to significant clues about what portfolio may be the best one when the investor does not have a clear idea about the level of risk he would tolerate.\n",
    "\n",
    "As sawn since the beginning of this work, the model presented in this work includes the last two.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assets returns\n",
    "\n",
    "According to P. Morettin, the relative variance or the *simple liquidity return* $R_t$ of an asset is given by:\n",
    "\n",
    "$$Rt = \\frac{P_t - P_{t-1}}{P_{t-1}}$$\n",
    "\n",
    "\n",
    "As $R_t$ is defined as the expected return of an asset, generally expressed as a percent value, $r_t$ however is defined as the *return rate* of an asset, given by:\n",
    "\n",
    "$$r_t = ln(\\frac{P_t}{P_{t-1}})$$\n",
    "\n",
    "Therefore, a simple return $R_t$ is obtained by: \n",
    "\n",
    "$$ R_t = {e^{r_t}} - 1 $$\n",
    "\n",
    "\n",
    "The first term presented above $R_t$, is pretended to be used when the optimization takes place, while $r_t$ is the termo to be predicted with a GARCH model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern portafolio theory - Markowitz models\n",
    "\n",
    "Modern portfolio theory (MPT) or mean-variance analysis is a matematical framework for assembling a portafolio composed by several assets looking for maximizing the *expected return* for a given level of risk. The key insight regarding this framework is that an asset risk and return, should be assesed by how it contributes to a portafolio's overall risk and return, rather that by itself [1 - ref. wikipedia]. \n",
    "\n",
    "It is an extension of diversification in investing, where is assumed that owning different type of assets is less risky than owining just one type. Harry Markowitz introduced MPT in 1952, and such work led him to be awarded a with Nobel price in economics[2 - wikipedia].\n",
    "\n",
    "\n",
    "**Risk and expected return:**\n",
    "\n",
    "* **Portfolio Return: ** Is the proportion-weighted combination of the constitutent asset's return.\n",
    "\n",
    "* **Portfolio volatility: ** Is a function of the correlation $\\rho_ij$ of the component assets, for all asset pairs *(i,j)*.\n",
    "\n",
    "###  Mathematical model: \n",
    "\n",
    "**Expected return**\n",
    "\n",
    "$$E(R_p)=\\sum_{i}w_iE(R_i)$$\n",
    "\n",
    "Where $R_p$ is the return of the portfolio, $R_i$ is the return on asset $i$ ans $w_i$ is the weighting of component asset (or the proportion of asset $i$ in the portfolio)\n",
    "\n",
    "    \n",
    "**Portfolio return variance**\n",
    "\n",
    "$$\\sigma_p^2 = \\sum_{i}w_i^2\\sigma_i^2 + \\sum_{i}\\sum_{j}w_iw_j\\sigma_i\\sigma_j\\rho_{ij}$$\n",
    "\n",
    "\n",
    "Where $\\sigma$ is the standard deviation (sample) of the periodic returns on an asset, and $\\rho_{ij}$ is the correlation coefficient between the returns on assets i and j. Alternatively the expression can be written as:\n",
    "\n",
    "$$\\sigma_p^2 = \\sum_{i}\\sum_{j}w_iw_j\\sigma_i\\sigma_j\\rho_{ij}$$\n",
    "\n",
    "Where $\\rho_{ij} = 1$ for $i=j$, or\n",
    "\n",
    "$$\\sigma_p^2 = \\sum_{i}\\sum_{j}w_iw_j\\sigma_{ij}$$\n",
    "\n",
    "\n",
    "where $\\sigma_{ij} = \\sigma_i\\sigma_j\\rho_{ij}$ is the (sample) covariance of the periodic returns on the two assets, or alternatively denoted as $\\sigma(i,j), cov_{ij}$ or $cov(i,j)$.\n",
    "\n",
    "**Portfolio return volatility (standard deviation)**\n",
    "\n",
    "$$\\sigma_p = \\sqrt{\\sigma_p^2}$$\n",
    "\n",
    "\n",
    "The mathematical model definition of the portfolio volatility relies on an algebraic representation, expensive to calculate when the number of assets within the portfolio is big. In perspective, the expression tends to increment its size at a $2x + 1$ rate per asset:\n",
    "\n",
    "* 2 assets:\n",
    "\n",
    "$$\\sigma_p^2 = w_A^2\\sigma_A^2 + w_B^2\\sigma_B^2 + 2w_Aw_B\\sigma_A\\sigma_B\\rho_{AB}$$\n",
    "\n",
    "* 3 assets:\n",
    "\n",
    "$$\\sigma_p^2 = w_A^2\\sigma_A^2 + w_B^2\\sigma_B^2 + w_C^2\\sigma_C^2 + 2w_Aw_B\\sigma_A\\sigma_B\\rho_{AB} + 2w_Aw_C\\sigma_A\\sigma_C\\rho_{AC} + 2w_Bw_C\\sigma_B\\sigma_C\\rho_{BC}$$\n",
    "\n",
    "\n",
    "Then one of the main contribution of Markowitz with is work, is the representation of these expression in a matricial form, which is computationally much cheaper.\n",
    "\n",
    "$$\n",
    "\\sigma_p^2 = \\left(\\begin{array}{cc} \n",
    "a_1 &  a_2 & ... & a_n\n",
    "\\end{array}\\right).\n",
    "\\left(\\begin{array}{cc} \n",
    "\\sigma^2(A_{11}) & \\sigma^2(A_{12})  & ... & \\sigma^2(A_{1n}) \\\\\n",
    "\\sigma^2(A_{21}) & \\sigma^2(A_{22})  & ... & \\sigma^2(A_{2n}) \\\\\n",
    "... & ...  & ... & ... \\\\\n",
    "\\sigma^2(A_{m1}) & \\sigma^2(A_{m2})  & ... & \\sigma^2(A_{mn}) \\\\\n",
    "\\end{array}\\right).\n",
    "\\left(\\begin{array}{cc} \n",
    "a_1\\\\ \n",
    "a_2\\\\\n",
    "...\\\\\n",
    "a_n\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "....\n",
    "\n",
    "Despite the theorical importance of this model, criticisms in considering this is as a suitable tool for real-world investments have been pointed out in many ways because of the mismatching in modeling accurately the real world features.\n",
    "\n",
    "Fundamentally, two issues are worth to remark, firstly the inhability of MPT in considering the key parameters from the data that might explain the root causes that yield to a certain effect or behaviour, and secondly, as either the risk and correlation are based on *expected values*, very often they fail to take acocunt of new circumstances that did not exist when the historical data were generated.\n",
    "\n",
    "Nevertheless, for the purpose of this work, MTP is a enough for measuring and comprehend the real behaviour of the crypto in portfolios during the last months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GARCH models - Getting return value of an asset by using \n",
    "\n",
    "[moretin]\n",
    "\n",
    "The efficient market theory indicates the prices of the returns incorporate the information available to all the stakeholders in the market, implyig that the variations for the prices are not predicatble (Fama, 1970; Campbell et. al, 1997). There are 3 categories of *efficency* depdending of the available information to the stakeholders:\n",
    "\n",
    "* **Weak efficiency:** Returns prices can not being predicted from past return prices. \n",
    "* **Semi-strong efficiency:** The basic set of data includes the history track of the return prices plus all the available public data. \n",
    "* **Strong efficiency:** The set of data includes all the public and private information. \n",
    "\n",
    "The \"weak efficiency\" scenario is very often to occur in stock prices prediction, hence by modeling the variance as a phenomena depending of time rather than the return price itself, is a valid approach. In order to model these more complex time series, models such as the called *stochastic volatility models*  are used. \n",
    "\n",
    "One of the most recalled type of these models is *ARCH* or *autoregressive conditional heteroscedasticity*, introduced by Engel in 1982 which aimed towards estimating the variance of the inflation phenomena in economics. This model stands by the over the fact a return $r_t$ is not serially correlated, but its conditional variance depends on past returns following a cuadratic function.\n",
    "\n",
    "$$ r_t = \\sqrt{h_t}\\epsilon_t$$\n",
    "$$ h_t = \\alpha_0 + \\alpha_1r_{t-1}^2 + ... + \\alpha_mr_{t-m}^2 $$\n",
    "\n",
    "$$ Where \\ \\epsilon_t \\ i.i.d \\ with \\  \\mu = 0, \\ \\alpha_0 > 0, \\alpha_i >, i = 1,..., m - 1, \\alpha_m > 0$$\n",
    "\n",
    "A generalization of this model was proposed by Bollesrlev (1986, 1987, 1988) called *GARCH* or *Generalized ARCH*. This model aims to also describe the variance yet it stands in the idea that the same variance might be modeled by fewer parameters than ARCH models, similar of what is achieved by using ARMA models instead of  a pure single AR or MA model.\n",
    "\n",
    "$$ r_t = \\sqrt{h_t}\\epsilon_t$$\n",
    "$$ h_t = \\alpha_0 + \\sum_{i=1}^{m}{\\alpha_ir_{t-i}^2} + \\sum_{j=1}^{n}{\\beta_jh_{t-j}} $$\n",
    "\n",
    "$$ Where \\ \\epsilon_t \\ i.i.d \\ with \\  \\mu = 0, \\ \\alpha_0 > 0, \\alpha_i >, i = 1,..., m - 1, \\ \\beta_j >= 0, \\ j = 1,...,n - 1, \\ \\alpha_m > 0, \\ \\beta_n > 0, \\ \\sum_{i=1}^{q}{(\\alpha_i+\\beta_j)} < 1, \\ q = max(m,n) $$\n",
    "\n",
    "$\\epsilon_t$ distribution follows either a normal or t-student distribution.\n",
    "\n",
    "Fitting a GARCH model to a time seris, is similar to fitting a time series using the ARIMA framework. Basically it is an optimization problem where is look to minimize a choosen parameter by testing several combinatins of parameters p, o, q. \n",
    "\n",
    "....\n",
    "\n",
    "Once it is possible to predict the value of $r_t$ we calculate the real return considering the expression (num_expr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Markowitz efficient frontier (P)\n",
    "\n",
    "\n",
    "Different from other proposals, this method will take advantage of the already simulated portafolios, trying to reach for the maximum return at each fixed risk value.\n",
    "\n",
    "For this, it is supposed that the portfolio with the maximum return and te portafolio with the minimal risk belong to the Markowitz efficient frontier, furthermore these ones may allow us to determine the interval of risk values within the quoted frontier. \n",
    "\n",
    "\n",
    "![Markowitz frontier illustration](./resources/img/markowitz_frontier.png \"Markowitz frontier illustration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the portfolios within Markowitz efficient frontier, Ratio Sharpe is the measure that will allow to pick the best one.\n",
    "\n",
    "\n",
    "Once the Markowitz frotnier is drawn, an investor is capable to pick the risk he will tolerate considering the probable return the portfolio might offer. But in many cases, if not the most, evaluate in which level of risk is desired to operate could result difficult to do.\n",
    "\n",
    "Therefore, the ratio of sharpe is introduced as an helpful criterion to overcome such difficulties.  \n",
    "\n",
    "This metric considers a free-risk asset in order to involve cost of opportunity (similar to ROI value).\n",
    "\n",
    "\n",
    "\n",
    "$$ Ratio \\ of \\ Sharpe = \\frac{𝑟_𝑝 − 𝑟_𝑓}{\\sigma} $$\n",
    "\n",
    "Where $\\sigma = \\sqrt{Variance(𝑟_𝑝 − 𝑟_𝑓)}$ ; if $𝑟_𝑓 $ is risk-free, then: $\\sigma = \\sigma_p$\n",
    "\n",
    "Finally \n",
    "\n",
    "$$ Ratio \\ of \\ Sharpe = \\frac{𝑟_𝑝 − 𝑟_𝑓}{\\sigma_p} $$\n",
    "\n",
    "\n",
    "Further work could be leverage towards testing a modified Ratio of Sharpe considering VaR instead of the portfolio variance. This remains out of the scope of this work due it was demostrated that using either the normal Ratio of Shape or the modified Ratio of Shape, does not represent any significant gain in deveolping markets such as the Colombian one where the volatility is higher. This scenario is different in already developed markets such as the american where using VaR instead of variance does delivers siginificant improvements, therefore testing how this modified version of such ratio works in the Brazilian market, is a future work.\n",
    "\n",
    "For the sake of simplicity it is assumed that Brazilian market and Colombian market are more similar to each other in terms of volatility compared with the similarity between American and Brazilian market in the same context.\n",
    "\n",
    "\n",
    "$$ Modified \\ Ratio \\ of \\ Sharpe = \\frac{𝑟_𝑝 − 𝑟_𝑓}{𝑉𝑎𝑅} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinatory Optimization problems and computational complexity \n",
    "\n",
    "One of the main challenges within combinatory optimization and searching problems is finding for the best solution (or global maximum/minimum depending of the problem to solve) among the set of the possible ones. In most cases, the mentioned set (or hyperspace) is composed by a countless number of possible solutions where reaching at the best one (or at least a suitable solution) requires certain intelligence or methodology rather than go through all the hyperspace which sometimes may require several years.\n",
    "\n",
    "These type of combinatory problems often are solved by using computational algorithms and they usually fit inside the **non-deterministic polynomial** problems (NP) family mainly because of its high complexity in execution time.\n",
    "\n",
    "Thus a problem could be fitted within the following categories regarding *computational complexity in execution time*:\n",
    "\n",
    "* **P type (polynomial):** Problems are solved by algorithms that deliver a solution in polynomial time (the ammount of time is possible to describe by a polynomial).\n",
    "\n",
    "* **NP type (non-deterministic polynomial):** Problems are solved by algorithms based on enumeration, nevertheless it is not possible to solve them in a reasonable time if the number of inputs is big.\n",
    "\n",
    "* **NP-Complete:** Strong evidence of the non existance of an algorithim whose *execution time* is a polynomial function depending of the input size.\n",
    "\n",
    "* ** NP-Hard: ** Optimization problems whose \"decision problems\" are *NP-Complete*.\n",
    "\n",
    "\n",
    "![Problems regarding *computational complexity in execution time*](./resources/img/complexity_time.png \"Problems regarding *computational complexity in execution time*\")\n",
    "\n",
    "\n",
    "Algorithms and techiques based on specific phenomena and heuristics  were devolped by some computer scientist gurus in order to deal with the *Acceptable execution time* and *a good solution* tradeoff when a problem belongs to one of the categories mentioned above. Between those techiques is possible to find algorithims based on entropy such as Simulated annhealing, evolutionary algorthims, multi agents-based, among others.\n",
    "\n",
    "..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithms\n",
    "\n",
    "Proposed by the mathematician and physiscs John Henry Holland, Ph.D. Genetic algorthms are based on the genetic processes that take place in organisms according to the principle of natural selection and evolution dictated by Darwin (1859). They work with a population of individuals where each one represents a possible solution to a given problem.  \n",
    "\n",
    "The GA power comes from the fact it is a robust technique capable of dealing successfully with a broad spectrum of problems in diverse areas, including the ones which find limitations when working with other techiques. While the GA don't gauarantee to reach for a global maximum, empirical evidence exists suggesting they usually find acceptable solutions in a very competitive time against other combinatory optimization algorithms[GA reference], nevertheless, for the type of problems where specializaed techiques for their resolution is known or exists, GA is not the best option, although in these mentioned scenarios hybrid approaches (specialized known techique plus GA) are more than suitable offering significant improvents regarding execution time and fitness degree.\n",
    "\n",
    "\n",
    "![GA workflow*](./resources/img/GA_workflow.png \"Problems regarding *GA workflow*\")\n",
    "\n",
    "\n",
    "## Codification\n",
    "\n",
    "It is assumed each indidivual could be represented as a set of parameters (denominated genes), when they are grouped it is formed a string of values often referred as *cromosomes*. While the used alphabet for representing an individual should not neccesarely br composed by {0,1}, most part of the theory which GA algorithms stand on, uses the mentioned alphabet.\n",
    "\n",
    "In biological terms, the set of parameters representing a *cromosome* is denominated *phenotype*. The *phenotype* contains the required information for build an organism (referred as genotype). The fitness of an individual to a certain problem depends of the genotype evaluation. \n",
    "\n",
    "This evaluation refers to a given score based on the fitness an indivual reaches to a certain *objective function* (e.x. In the nature is likely the hability to compete for resouces). The nature of this function depends on the specific problem to be maximized/minimized. \n",
    "\n",
    "During the *reproduction* phase, individuals are choosen for crossing themselves with each other and thus create the descendents or the individuals of the next generation. The parent selection then, is a probabilistic process described as a *selection process function* proportional to the *objective function* [ref tesis] and favoring the ones with the greater fitness to the problem. This process is based on the *skewed roulette* scheme which indicates the well fitted indivduals will be probably selected several times per generation, while the poorly fitted indivduals will be selected ocasionally.\n",
    "\n",
    "Once the a couple of parents is selected, their cromosomes are crossed with each others usually using *crossover* and *mutation* operators.\n",
    "\n",
    "The *crossover* operator, takes the parents strings and randomly cuts them in a specific point in order to produce a couple of initial (head) and a couple of final (tail) substrings. Subsequently the final substrings or tails are switched producing two new cromosomes. \n",
    "\n",
    "\n",
    "![Crossover operator based in a single point](./resources/img/crossover_one_point.png \"Crossover operator based in a single point\")\n",
    "\n",
    "This operator is known as *single point based crossover operator*, and frequenlty the decisition whether it is applied to a couple of parents or not, relies on a random process with a problility between 0.5 and 1.0 of being applied. In the case of the couples that weren't crossed by this process, their descendents are obtained just by duplicating them.\n",
    "\n",
    "The *mutation operator* is applied to each children individually. It consists in randomly alterating (normally with a low probability) each gen of the cromosome. \n",
    "\n",
    "![https://localwire.pl/graphinder-genetic-algorithm-mutation/](./resources/img/mutation.png \"Mutation operator\")\n",
    "\n",
    "Either both operators allow the algorithm to reach a balance between exploration-exploiting *tradeoff*. While the *convergence op.* advocates for maximizing the exploration of the hyperspace in higher but more superficial degree, the *mutation op.* guarantees a more detailed exploration over the hyperspce (exploiting). The later one is also the main responsible of the convergence degree of the algorithm.\n",
    "\n",
    "In practical terms the *convergence* definition given by De Jong (1975) in his doctoral thesis, turns out very useful: If the GA has been implemented correctly, the population will evolve increasing either the fitness of its best individual and the average fitness of all population towards the *optimal global* generation by generation.\n",
    "\n",
    "\n",
    "![convergence](./resources/img/convergence.png \"convergence\")\n",
    "\n",
    "This concept is related with the progression towards uniformity: A gen has converged when at least 95% of the individuals shared the same value for such gen. It is told a populations converges when all genes have converged but this definition can be generalized to a $\\beta \\%$ of converged individuals. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "## Gathering data\n",
    "\n",
    "Collecting the data of the stock prices often implies a considerable monetary cost. The Sao Paulo stock market B3 for instance, offers this as a service where the interested brokers pay for obtaining the updated state of the prices as fast as possible. Such data however, is realised and made public after 15 minutes of its generation which means the brokers partially pay for those 15 minutes of difference. The other intrinsic  services the brokers pay for are the  transmission channels the data is delivered in. Usually these type of solutions are customized and robust as they maximize reliability, availability and scalability. Exposed as APIs.\n",
    "\n",
    "In the case these services are not affordable, the challenge relies on devoloping adequate software tools for colecting the required data, when it is public. Considering either in stock and cryptocurrency market, this information is open through several financial websites such as Google Finance (Stocks), Yahoo finance (Stocks), Bloomberg (Stocks), Coinmarketcap (cryptocurrencies), etc. The main challenge stands in decoding and parsing it from pure HTML to the desired format.\n",
    "\n",
    "The approach used in this work is the same as the mentioned above, it relies on executing a sequence of HTTP request and then parsing the HTML code obatined from downloading the entire website using Python libraries such as *urllib3*, *beatifusoup* and *pandas*.\n",
    "\n",
    "**Web Datasorces: ** \n",
    "\n",
    "The main criterion for choosing the most adequate web datasource was the ease at automating the sequence of HTTP request and the consistency of the HTML components containing the relevant information.\n",
    "\n",
    "\n",
    "* **Yahoo finance: **  Among the anlysed web sources, mainly for brazilian stocks prices, the one that was easier to parse was *Yahoo Finance*. As specified earlier, the HTML was parsed in order to extract the relevant information.\n",
    "\n",
    "![yahoo web template](./resources/img/image_yahoo_template.png \"yahoo web template\")\n",
    "\n",
    "\n",
    "    * HTTP request: Each request (GET wikipedia) represents a stock historical price track,\n",
    "\n",
    "\n",
    "https://br.financas.yahoo.com/quote/{asset_symbol}/history?period1={initial_date}&period2={end_date}&interval=1d&filter=history&frequency=1d\n",
    "\n",
    "\n",
    "        * Paramters:\n",
    "            * asset_symbol: The symbol that identifies a certain company in the market (e.x. \"VIVT4.SA\").\n",
    "            * initial_date: Defines the starting point at which the considered time interval of the historical data is defined. \n",
    "            * end_date: Defines the ending point at which the considered time interval of the historical data is defined.\n",
    "        \n",
    "        In this case, either *initial_date* and *end_date* parameters require a long representation of the date or the ammount of seconds passed since 01-01-1970 until the specified date.\n",
    "\n",
    "Yahoo website also presented certain limitations when was intended to download an extense stock historical track. It was perceived that intervals above 150 days (e.x. init_date = '01-01-2018' and end_date = '01-05-2018') do not work correctly as is only displayed part of the information. Therefore the download process was splitted in chunks of 90 days-size per stock.\n",
    "\n",
    "![download flow](./resources/img/download_flux.png \"download flow\")\n",
    "\n",
    "Each iteration outcome is a 90 days-sized pandas dataframe, where later are merged together (*union*) in order to consolidate the history track as one.\n",
    "\n",
    "![Stock Pandas Dataframe](./resources/img/stock_pandas.png \"Stock Pandas Dataframe\")\n",
    "\n",
    "As sawn in the yahoo web, the date format within the table contains the dates naming the months with portuguese prefixes (e.x. *03 de ago de 2018*), hence a work was done by re-mapping such format to a more generic one (the sawn at the pandas dataframe image) aiming to facilitate the merge process to come (detailed later).\n",
    "\n",
    "* **Coinmarketcap: ** Similarly to Yahoo finance, Coinmarketcap implements GET requests, furthemore issues as the detected in the Yahoo website, were not found in it.\n",
    "\n",
    "![Coinmarketcap template](./resources/img/coinmarket_cap_template.png \"Coinmarketcap template\")\n",
    "\n",
    "* HTTP request: Each request (GET wikipedia) represents a cryptocurrencie historical price track,\n",
    "\n",
    "\n",
    "https://coinmarketcap.com/currencies/{crypto_name}/historical-data/?start=20170101&end=20180731\n",
    "\n",
    "\n",
    "        * Paramters:\n",
    "            * crypto_name: The name of the cryptocurrencie (e.x. \"Ethereum\").\n",
    "            * start: Defines the starting point at which the considered time interval of the historical data is defined. \n",
    "            * end: Defines the ending point at which the considered time interval of the historical data is defined.\n",
    "        \n",
    "Different of Yahoo, in this case either *start* and *end* are represented by a more legible date format: *YearMonthDay* (e.x. 2018-08-31). Aditionally there was not need of splitting the download flow in chunks, coinmarketcap is capable to display all the data without implicit additional restrictions besides the date filters.\n",
    "\n",
    "![Cryptocurrencies download flow](./resources/img/crypto_download_flow.png \"Cryptocurrencies download flow\")\n",
    "\n",
    "Hence, the outcome dataframe structure shares the same format the stock pandas dataframe has, considering a similar work regarding re-mapping the date format in coinmarketcap  was also implemented.\n",
    "\n",
    "![Crypto Pandas Dataframe](./resources/img/crypto_pandas.png \"Crypto Pandas Dataframe\")\n",
    "\n",
    "\n",
    "## Merging the data:\n",
    "\n",
    "By considereing the date column as the index of both dataframe, a simply join between both of them generated was done.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "\n",
    "- nan values\n",
    "- historical divergences when a crypti has not enoguh data regarding the defined window time\n",
    "- diff series\n",
    "\n",
    "## Time series pre-processing\n",
    "\n",
    "Considering the expression (number), by logarithm propierties we have\n",
    "\n",
    "$$r_t = ln(P_t) - ln(P_{t-1})$$\n",
    "\n",
    "As it was previously defined, $r_t$ or *return rate* represents therefore, a first order derivate of the logarithms of the prices. Usually this is applyed in order to avoid biases caused by the non-stationary in trend and variance as they are difficult to model.\n",
    "\n",
    "Recapitulating, a *stationary process* is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time.\n",
    "\n",
    "![stationarity comparison](./resources/img/stationary_comparison.png \"stationarity comparison\")\n",
    "\n",
    "In order to check whether a time series is stationary or not, Augmented Dickey–Fuller test is applied. The augmented Dickey–Fuller (ADF) statistic, used in the test, is a negative number the more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence [1 wikipedia ref]. Hence, the tresholds for rejecting the null hypothsis (based on the t-student distribution) are given by the table below [2 ref]. \n",
    "\n",
    "![dickey_fuller critical values for t-student ditribution](./resources/img/df_critical_values_tstudent.png \"dickey_fuller critical values for t-student ditribution\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### (P) hacer data frame calculando todos los estaditsicos ADF de todas las series\n",
    "\n",
    "\n",
    "By transforming the asset's time series in such a way, the GARCH modeling process is improved due the shrinking in the optimization grid space (or the number of the possbile combinations to test), as is going to be explained in sections to come.\n",
    "\n",
    "\n",
    "## NaN values\n",
    "\n",
    "The NaN values prescence in the downloaded data was mainly due unexpcted issues such the lack of the complete history track (from 01-01-2017 until 31-07-2018) of some cryptocurrencues, and the lack of data regarding weekends and holydays for stock data.\n",
    "\n",
    "The NaN vallues were filled as '0' after applying the first transformation of the closing price series. Considering the nature of the first order derivated series, a '0' value means there is no any change between the series at $t$ and at $ t-1 $. Is the same as filling the missing price at $t$ with the $t-1$ value in the original closing price series.\n",
    "\n",
    "Such strategy avoids to increase or decrease the value of the mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \tInsights and assumptions:\n",
    "* Negative correlation between crypto=currencies and stock market assets.\n",
    "* Assets selection\n",
    "    * Pareto’s law, considering just most valuable companies operating in the market in order to avoid undesired effects like “toroid” and “…” (Check with Rodrigo the name of these effects).\n",
    "    * The same applied to Criptocurrencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
